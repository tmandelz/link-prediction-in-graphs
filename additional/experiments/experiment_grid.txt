
Hyperparametertuning
	GCN
		Reference Hyperparametertune
			gnn.py 
			--project_name TUNE-GCN-Reference 
			--run_name 1_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid
			{"batch_size": {"max": 262144, "min": 32768, "scaling_type": "linear", "type": "integer"}, 
			"hidden_channels": {"scaling_type": "uniform", "type": 	"discrete", "values": [128, 256, 384, 512]}, 
			"lr": {"max": 0.001, "min": 0.0001, "scaling_type": "linear", "type": "double"}, 
			"num_layers": {"max": 3, "min": 	2, "scaling_type": "uniform", "type": "integer"}}

		Pretrain Hyperparametertuning
			gnn.py
			--project_name TUNE-GCN-Pretrain
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-papers100M 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  -> TODO defined by Reference Size
			--num_layers -> TODO defined by Reference Size
			--parameter_tuning_param_grid
			{"batch_size": {"max": 65536, "min": 32768, "scaling_type": "linear", "type": "integer"}, 
			"lr": {"max": 0.0005, "min": 0.0001, "scaling_type": "linear", "type": "double"}}

		Finetune Hyperparametertune
			gnn.py
			--project_name TUNE-GCN-Finetune
			--run_name 1_TUN_RUN
			--epochs 25 
			--dataset ogbn-arxiv 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  -> TODO defined by Reference Size
			--num_layers -> TODO defined by Reference Size
			--model_path -> TODO defined by best model pretrain
			--predictor_path -> TODO defined by best model pretrain
			--parameter_tuning_param_grid
			{"batch_size": {"max": 57520, "min": 25566, "scaling_type": "linear", "type": "integer"}, 
			"lr": {"max": 0.0005, "min": 0.0001, "scaling_type": "linear", "type": "double"}}
	GraphSage (NEEDS CHECKs if paramater are more or less good!!!)
		Reference Hyperparametertune
			gnn.py 
			--project_name TUNE-SAGE-Reference 
			--run_name 1_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid
			{"batch_size": {"max": 262144, "min": 32768, "scaling_type": "linear", "type": "integer"}, 
			"hidden_channels": {"scaling_type": "uniform", "type": 	"discrete", "values": [128, 256, 384, 512]}, 
			"lr": {"max": 0.001, "min": 0.0001, "scaling_type": "linear", "type": "double"}, 
			"num_layers": {"max": 3, "min": 	2, "scaling_type": "uniform", "type": "integer"}}

		Pretrain Hyperparametertuning
			gnn.py
			--project_name TUNE-SAGE-Pretrain 
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-papers100M 
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  -> TODO defined by Reference Size
			--num_layers -> TODO defined by Reference Size
			--parameter_tuning_param_grid
			{"batch_size": {"max": 65536, "min": 32768, "scaling_type": "linear", "type": "integer"}, 
			"lr": {"max": 0.0005, "min": 0.0001, "scaling_type": "linear", "type": "double"}}

		Finetune Hyperparametertune
			gnn.py
			--project_name TUNE-SAGE-Finetune 
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-arxiv  
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  -> TODO defined by Reference Size
			--num_layers -> TODO defined by Reference Size
			--model_path -> TODO defined by best model pretrain
			--predictor_path -> TODO defined by best model pretrain
			--parameter_tuning_param_grid
			{"batch_size": {"max": 57520, "min": 25566, "scaling_type": "linear", "type": "integer"}, 
			"lr": {"max": 0.0005, "min": 0.0001, "scaling_type": "linear", "type": "double"}}
	GIN (NEEDS CHECKs if paramater are more or less good!!!)
		Reference Hyperparametertune
			gnn.py 
			--project_name TUNE-GIN-Reference 
			--run_name 1_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid
			{"batch_size": {"max": 262144, "min": 32768, "scaling_type": "linear", "type": "integer"}, 
			"hidden_channels": {"scaling_type": "uniform", "type": 	"discrete", "values": [128, 256, 384, 512]}, 
			"lr": {"max": 0.001, "min": 0.0001, "scaling_type": "linear", "type": "double"}, 
			"num_layers": {"max": 3, "min": 	2, "scaling_type": "uniform", "type": "integer"}}

		Pretrain Hyperparametertuning
			gnn.py
			--project_name TUNE-GIN-Pretrain 
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-papers100M 
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  -> TODO defined by Reference Size
			--num_layers -> TODO defined by Reference Size
			--parameter_tuning_param_grid
			{"batch_size": {"max": 65536, "min": 32768, "scaling_type": "linear", "type": "integer"}, 
			"lr": {"max": 0.0005, "min": 0.0001, "scaling_type": "linear", "type": "double"}}

		Finetune Hyperparametertune
			gnn.py
			--project_name TUNE-GIN-Finetune 
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-arxiv  
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  -> TODO defined by Reference Size
			--num_layers -> TODO defined by Reference Size
			--model_path -> TODO defined by best model pretrain
			--predictor_path -> TODO defined by best model pretrain
			--parameter_tuning_param_grid
			{"batch_size": {"max": 57520, "min": 25566, "scaling_type": "linear", "type": "integer"}, 
			"lr": {"max": 0.0005, "min": 0.0001, "scaling_type": "linear", "type": "double"}}

Train optimal models
	GCN
		Reference
			gnn.py 
			--project_name GNN_models 
			--run_name Reference_GCN 
			--epochs 2100 -> TODO Define length of training
			--dataset ogbn-arxiv 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
		Pretrain
			gnn.py 
			--project_name GNN_models 
			--run_name Pretrain_GCN 
			--epochs 200 -> TODO Define length of training
			--dataset papers100M 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5

		Finetune
			gnn.py 
			--project_name GNN_models 
			--run_name Finetune_GCN 
			--epochs 2100 -> TODO Define length of training
			--dataset ogbn-arxiv 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
	GraphSage
		Reference
			gnn.py 
			--project_name GNN_models 
			--run_name Reference_SAGE
			--epochs 2100 -> TODO Define length of training
			--dataset ogbn-arxiv 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
		Pretrain
			gnn.py 
			--project_name GNN_models 
			--run_name Pretrain_SAGE 
			--epochs 200 -> TODO Define length of training
			--dataset papers100M 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5

		Finetune
			gnn.py 
			--project_name GNN_models 
			--run_name Finetune_SAGE 
			--epochs 2100 -> TODO Define length of training
			--dataset ogbn-arxiv 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture SAGE
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
	GIN
		Reference
			gnn.py 
			--project_name GNN_models 
			--run_name Reference_GIN
			--epochs 2100 -> TODO Define length of training
			--dataset ogbn-arxiv 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
		Pretrain
			gnn.py 
			--project_name GNN_models 
			--run_name Pretrain_GIN 
			--epochs 200 -> TODO Define length of training
			--dataset papers100M 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5

		Finetune
			gnn.py 
			--project_name GNN_models 
			--run_name Finetune_GIN
			--epochs 2100 -> TODO Define length of training
			--dataset ogbn-arxiv 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5

Fehlerabsch√§tzung multiple runs (for each model?)