
Hyperparametertuning:
	GCN:
		Reference Hyperparametertune:
			bash additional/i4ds_cluster/train_models.sh 
			--project_name TUNE-GCN-Reference 
			--run_name 1_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-GCN-Reference_1_TUNE_RUN.json

			bash additional/i4ds_cluster/train_models.sh 
			--project_name TUNE-GCN-Reference 
			--run_name 2_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-GCN-Reference_2_TUNE_RUN.json
			
			bash additional/i4ds_cluster/train_models.sh 
			--project_name TUNE-GCN-Reference 
			--run_name 3_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-GCN-Reference_3_TUNE_RUN.json
		
		Pretrain Hyperparametertuning:
			bash additional/i4ds_cluster/train_models_big_gpus.sh
			--project_name TUNE-GCN-Pretrain
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-papers100M 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels 384
			--num_layers 2
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-GCN-Pretrain_1_TUNE_RUN.json
		Finetune Hyperparametertune:
			bash additional/i4ds_cluster/train_models.sh
			--project_name TUNE-GCN-Finetune
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-arxiv 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  384
			--num_layers 2
			--model_path -> TODO defined by best model pretrain
			--predictor_path -> TODO defined by best model pretrain
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-GCN-Finetune_1_TUNE_RUN.json
	GraphSage (NEEDS CHECKs if paramater are more or less good!!!)
		Reference Hyperparametertune:
			bash additional/i4ds_cluster/train_models.sh 
			--project_name TUNE-SAGE-Reference 
			--run_name 1_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-SAGE-Reference_1_TUNE_RUN.json

			bash additional/i4ds_cluster/train_models.sh 
			--project_name TUNE-SAGE-Reference 
			--run_name 2_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-SAGE-Reference_2_TUNE_RUN.json

			bash additional/i4ds_cluster/train_models.sh 
			--project_name TUNE-SAGE-Reference 
			--run_name 3_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-SAGE-Reference_3_TUNE_RUN.json
		

		Pretrain Hyperparametertuning:
			bash additional/i4ds_cluster/train_models_big_gpus.sh
			--project_name TUNE-SAGE-Pretrain 
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-papers100M 
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  384
			--num_layers 2
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-SAGE-Pretrain_1_TUNE_RUN.json

		Finetune Hyperparametertune:
			bash additional/i4ds_cluster/train_models.sh
			--project_name TUNE-SAGE-Finetune 
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-arxiv  
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  384
			--num_layers 2
			--model_path -> TODO defined by best model pretrain
			--predictor_path -> TODO defined by best model pretrain
			--parameter_tuning_param_grid
			{"batch_size": {"max": 57520, "min": 25566, "scaling_type": "linear", "type": "integer"}, 
			"lr": {"max": 0.0005, "min": 0.0001, "scaling_type": "linear", "type": "double"}}
	GIN (NEEDS CHECKs if paramater are more or less good!!!)
		Reference Hyperparametertune
			bash additional/i4ds_cluster/train_models.sh 
			--project_name TUNE-GIN-Reference 
			--run_name 1_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-GIN-Reference_1_TUNE_RUN.json

			bash additional/i4ds_cluster/train_models.sh 
			--project_name TUNE-GIN-Reference 
			--run_name 2_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-GIN-Reference_2_TUNE_RUN.json
			
			bash additional/i4ds_cluster/train_models.sh 
			--project_name TUNE-GIN-Reference 
			--run_name 3_TUNE_RUN
			--epochs 150 
			--dataset ogbn-arxiv 
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--parameter_tuning_param_grid ./modelling/gnn/parameter_tuning_param_grid_TUNE-GIN-Reference_3_TUNE_RUN.json

		Pretrain Hyperparametertuning
			bash additional/i4ds_cluster/train_models_big_gpus.sh
			--project_name TUNE-GIN-Pretrain 
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-papers100M 
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  -> TODO defined by Reference Size
			--num_layers -> TODO defined by Reference Size
			--parameter_tuning_param_grid
			{"batch_size": {"max": 65536, "min": 32768, "scaling_type": "linear", "type": "integer"}, 
			"lr": {"max": 0.0005, "min": 0.0001, "scaling_type": "linear", "type": "double"}}

		Finetune Hyperparametertune:
			bash additional/i4ds_cluster/train_models.sh
			--project_name TUNE-GIN-Finetune 
			--run_name 1_TUNE_RUN
			--epochs 25 
			--dataset ogbn-arxiv  
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model False 
			--eval_n_hop_computational_graph 0 
			--hidden_channels  -> TODO defined by Reference Size
			--num_layers -> TODO defined by Reference Size
			--model_path -> TODO defined by best model pretrain
			--predictor_path -> TODO defined by best model pretrain
			--parameter_tuning_param_grid
			{"batch_size": {"max": 57520, "min": 25566, "scaling_type": "linear", "type": "integer"}, 
			"lr": {"max": 0.0005, "min": 0.0001, "scaling_type": "linear", "type": "double"}}



Train optimal models:
	GCN:
		Reference:
			Reference:
			bash additional/i4ds_cluster/train_models.sh
			--project_name Long-Reference
			--run_name Reference_GCN
			--epochs 2100
			--dataset ogbn-arxiv
			--batch_size 52109
			--lr 0.000686411
			--num_layers 2
			--hidden_channels 384
			--model_architecture GCN
			--one_batch_training False
			--freeze_model False
			--save_model True
			--eval_n_hop_computational_graph 0
			--epoch_checkpoints 5
		Pretrain:
			bash additional/i4ds_cluster/train_models_big_gpus.sh 
			--project_name GNN_models 
			--run_name Pretrain_GCN 
			--epochs 200 -> TODO Define length of training
			--dataset papers100M 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
		Finetune:
			bash additional/i4ds_cluster/train_models.sh 
			--project_name GNN_models 
			--run_name Finetune_GCN 
			--epochs 2100 -> TODO Define length of training
			--dataset ogbn-arxiv 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture GCN 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
	SAGE:
		Reference:
			bash additional/i4ds_cluster/train_models.sh 
			--project_name GNN_models 
			--run_name Reference_SAGE
			--epochs 2100 -> TODO Define length of training
			--dataset ogbn-arxiv 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
		Pretrain:
			bash additional/i4ds_cluster/train_models_big_gpus.sh 
			--project_name GNN_models 
			--run_name Pretrain_SAGE 
			--epochs 200 -> TODO Define length of training
			--dataset papers100M 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture SAGE 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
		Finetune:
			bash additional/i4ds_cluster/train_models.sh 
			--project_name GNN_models 
			--run_name Finetune_SAGE 
			--epochs 2100 -> TODO Define length of training
			--dataset ogbn-arxiv 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture SAGE
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
	GIN:
		Reference:
			bash additional/i4ds_cluster/train_models.sh
			--project_name long-reference
			--run_name Reference_GIN
			--epochs 2100
			--dataset ogbn-arxiv
			--batch_size 33134
			--lr 0.000284653
			--num_layers 2
			--hidden_channels 384
			--model_architecture GIN
			--one_batch_training False
			--freeze_model False
			--save_model True
			--eval_n_hop_computational_graph 0
			--epoch_checkpoints 5
		Pretrain:
			bash additional/i4ds_cluster/train_models_big_gpus.sh 
			--project_name GNN_models 
			--run_name Pretrain_GIN 
			--epochs 200 -> TODO Define length of training
			--dataset papers100M 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5
		Finetune:
			bash additional/i4ds_cluster/train_models.sh 
			--project_name GNN_models 
			--run_name Finetune_GIN
			--epochs 2100 -> TODO Define length of training
			--dataset ogbn-arxiv 
			--batch_size  -> TODO Defined trough Hyperparametertune 
			--lr  -> TODO Defined trough Hyperparametertune 
			--num_layers  -> TODO Defined trough Hyperparametertune 
			--hidden_channels -> TODO Defined trough Hyperparametertune 
			--model_architecture GIN 
			--one_batch_training False 
			--freeze_model False 
			--save_model True 
			--eval_n_hop_computational_graph 0 
			--epoch_checkpoints 5

Fehlerabschätzung multiple runs (for each model?)